import os
import requests
import pytesseract
import tempfile
from bs4 import BeautifulSoup
from langdetect import detect, DetectorFactory
from PIL import Image
from io import BytesIO
import textract
import pandas as pd

# –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ langdetect
DetectorFactory.seed = 0

SUPPORTED_EXTENSIONS = ['.txt', '.pdf', '.docx']

# üí¨ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞
def detect_language(text):
    try:
        lang = detect(text)
    except:
        lang = "unknown"
    return lang

# üì§ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
def extract_text(input_type, input_data, lang_hint=None):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    
    :param input_type: —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: 'text', 'file', 'url', 'image'
    :param input_data: –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (—Å—Ç—Ä–æ–∫–∞, —Ñ–∞–π–ª, URL –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
    :param lang_hint: –ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
    :return: –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
    """
    if input_type == "text":
        return input_data
    elif input_type == "file":
        return load_text_from_file(input_data)
    elif input_type == "url":
        return load_text_from_url(input_data)
    elif input_type == "image":
        lang_code = lang_hint or "eng"
        return load_text_from_image(input_data, lang=lang_code)
    else:
        return "‚ùå Unknown input type"

# üì§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
def load_text_from_file(uploaded_file):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ (.txt, .pdf, .docx).
    """
    file_ext = os.path.splitext(uploaded_file.name)[-1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    try:
        if file_ext == ".txt":
            with open(tmp_path, "r", encoding="utf-8", errors="ignore") as f:
                return f.read()
        elif file_ext in [".pdf", ".docx"]:
            return textract.process(tmp_path).decode("utf-8", errors="ignore")
        else:
            return f"‚ùå Unsupported file extension: {file_ext}"
    except Exception as e:
        return f"‚ùå Error extracting file text: {e}"
    finally:
        os.remove(tmp_path)

# üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å URL
def load_text_from_url(url):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL.
    """
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏ –∏ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π HTML
        for tag in soup(["script", "style", "noscript"]):
            tag.extract()

        # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator="\n")
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return "\n".join(lines)
    except Exception as e:
        return f"‚ùå Error fetching URL content: {e}"

# üñºÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
def load_text_from_image(uploaded_image, lang="eng"):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OCR.
    """
    try:
        image = Image.open(uploaded_image).convert("RGB")
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º OCR
        gray = image.convert("L")
        text = pytesseract.image_to_string(gray, lang=lang)
        return text.strip()
    except Exception as e:
        return f"‚ùå Error processing image: {e}"

